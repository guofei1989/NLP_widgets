{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "利用sklearn中的feature_extraction进行OneHot向量化<br>\n",
    "OneHot向量化基于词袋模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 英文文本的处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CountVectorizer(input='content', encoding='utf-8',\n",
    "                 decode_error='strict', strip_accents=None,\n",
    "                 lowercase=True, preprocessor=None, tokenizer=None,\n",
    "                 stop_words=None, token_pattern=r\"(?u)\\b\\w\\w+\\b\",\n",
    "                 ngram_range=(1, 1), analyzer='word',\n",
    "                 max_df=1.0, min_df=1, max_features=None,\n",
    "                 vocabulary=None, binary=False, dtype=np.int64)\n",
    "常用参数说明：\n",
    "input：序列对象，序列元素可以为文件地址、文件指针或文本\n",
    "lowercase: 对于英文，默认先进行lower处理\n",
    "tokenizer: 分词器，None或callable对象，在analyzer='word'时可覆写\n",
    "stop_words: 停用词\n",
    "token_pattern: 分词正则表达式，作用和分词器一样, 默认为r\"(?u)\\b\\w\\w+\\b\"，即至少两个字符\n",
    "ngram_range: 采用n-gram模型的范围\n",
    "analyzer: word级别或char级别, 具体有 单词为单位的word， 字符为单位的char， 以及先预先进行word再在word范围内进行char级别的n-gram划分\n",
    "max_df: 最大文档频率， 若vocabulary=None忽略该参数\n",
    "min_df: 最小文档频率， 若vocabulary=None忽略该参数\n",
    "max_features：最大特征个数\n",
    "vocabulary：可指定词典{word: index}, 从而只对感兴趣的词语进行向量化，默认对所有的input内容进行向量化\n",
    "binary: 默认False, 即count会累计计数；若为True，出现则为1，不计具体count\n",
    "\"\"\"\n",
    "corpus = [\"I come to China to travel\",\n",
    "    \"This is a car polupar in China\",\n",
    "    \"I love tea and Apple \",\n",
    "    \"The work is to write some papers in science\"]\n",
    "\n",
    "\n",
    "vector = CountVectorizer()\n",
    "result = vector.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and', 'apple', 'car', 'china', 'come', 'in', 'is', 'love', 'papers', 'polupar', 'science', 'some', 'tea', 'the', 'this', 'to', 'travel', 'work', 'write']\n",
      "[(0, 'and'), (1, 'apple'), (2, 'car'), (3, 'china'), (4, 'come'), (5, 'in'), (6, 'is'), (7, 'love'), (8, 'papers'), (9, 'polupar'), (10, 'science'), (11, 'some'), (12, 'tea'), (13, 'the'), (14, 'this'), (15, 'to'), (16, 'travel'), (17, 'work'), (18, 'write')]\n"
     ]
    }
   ],
   "source": [
    "# 特征名称，即word. 这是vector对象的方法\n",
    "print(vector.get_feature_names())\n",
    "\n",
    "features_list = []\n",
    "for i, j in enumerate(vector.get_feature_names()):\n",
    "    features_list.append((i,j))\n",
    "print(features_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 16)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 15)\t2\n",
      "  (0, 4)\t1\n",
      "  (1, 5)\t1\n",
      "  (1, 9)\t1\n",
      "  (1, 2)\t1\n",
      "  (1, 6)\t1\n",
      "  (1, 14)\t1\n",
      "  (1, 3)\t1\n",
      "  (2, 1)\t1\n",
      "  (2, 0)\t1\n",
      "  (2, 12)\t1\n",
      "  (2, 7)\t1\n",
      "  (3, 10)\t1\n",
      "  (3, 8)\t1\n",
      "  (3, 11)\t1\n",
      "  (3, 18)\t1\n",
      "  (3, 17)\t1\n",
      "  (3, 13)\t1\n",
      "  (3, 5)\t1\n",
      "  (3, 6)\t1\n",
      "  (3, 15)\t1\n"
     ]
    }
   ],
   "source": [
    "# reulst的数据类型为scipy.sparse.csr.csr_matrix， 每个元素为   (document_id, word_id)  count\n",
    "# 因为token_pattern默认至少两个字符，所以I、a忽略\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 2 1 0 0]\n",
      " [0 0 1 1 0 1 1 0 0 1 0 0 0 0 1 0 0 0 0]\n",
      " [1 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 1 0 1 0 1 1 0 1 0 1 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "# 矩阵\n",
    "print(result.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 'and'), (1, 'and apple'), (2, 'apple'), (3, 'car'), (4, 'car polupar'), (5, 'china'), (6, 'china to'), (7, 'come'), (8, 'come to'), (9, 'in'), (10, 'in china'), (11, 'in science'), (12, 'is'), (13, 'is car'), (14, 'is to'), (15, 'love'), (16, 'love tea'), (17, 'papers'), (18, 'papers in'), (19, 'polupar'), (20, 'polupar in'), (21, 'science'), (22, 'some'), (23, 'some papers'), (24, 'tea'), (25, 'tea and'), (26, 'the'), (27, 'the work'), (28, 'this'), (29, 'this is'), (30, 'to'), (31, 'to china'), (32, 'to travel'), (33, 'to write'), (34, 'travel'), (35, 'work'), (36, 'work is'), (37, 'write'), (38, 'write some')]\n"
     ]
    }
   ],
   "source": [
    "# ngram_range影响统计的n_gram\n",
    "vector1 = CountVectorizer(analyzer='word', ngram_range=(1,2))   # 会考虑连续两个词的特征\n",
    "result1 = vector1.fit_transform(corpus)\n",
    "\n",
    "features_list1 = []\n",
    "for i, j in enumerate(vector1.get_feature_names()):\n",
    "    features_list1.append((i,j))\n",
    "print(features_list1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 0, 1, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0,\n",
       "        0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
       "        0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1,\n",
       "        1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result1.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 'and'), (1, 'china'), (2, 'in'), (3, 'is'), (4, 'some'), (5, 'tea'), (6, 'the'), (7, 'this'), (8, 'to'), (9, 'travel')]\n"
     ]
    }
   ],
   "source": [
    "# max_features参数的影响, 会对one-hot维度进行截断\n",
    "vector2 = CountVectorizer(max_features=10)\n",
    "result2 = vector2.fit_transform(corpus)\n",
    "\n",
    "features_list = []\n",
    "for i, j in enumerate(vector2.get_feature_names()):\n",
    "    features_list.append((i,j))\n",
    "print(features_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 9)\t1\n",
      "  (0, 1)\t1\n",
      "  (0, 8)\t2\n",
      "  (1, 2)\t1\n",
      "  (1, 3)\t1\n",
      "  (1, 7)\t1\n",
      "  (1, 1)\t1\n",
      "  (2, 0)\t1\n",
      "  (2, 5)\t1\n",
      "  (3, 4)\t1\n",
      "  (3, 6)\t1\n",
      "  (3, 2)\t1\n",
      "  (3, 3)\t1\n",
      "  (3, 8)\t1\n"
     ]
    }
   ],
   "source": [
    "print(result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 0 0 0 0 0 2 1]\n",
      " [0 1 1 1 0 0 0 1 0 0]\n",
      " [1 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 1 1 1 0 1 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "# 结果的array只有10维\n",
    "print(result2.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 中文文本的处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "中文相较于英文的一个显著区别在于：英文通过空格自动分词（如token_pattern=r\"(?u)\\b\\w\\w+\\b\"所示），所以在使用sklearn进行向量化的时候要先进行中文的分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/rk/nnl9yhm55kb6325ckffkn6hw0000gn/T/jieba.cache\n",
      "Loading model cost 0.680 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['今天天气 不错', '最近 工作 非常 忙', '中文 自然语言 处理 非常 男难', '今天 是 星期二']\n"
     ]
    }
   ],
   "source": [
    "corpus_cn = [\"今天天气不错\",\n",
    "    \"最近工作非常忙\",\n",
    "    \"中文自然语言处理非常男难\",\n",
    "    \"今天是星期二\"]\n",
    "\n",
    "def word_segment(sent):\n",
    "    return \" \".join(jieba.cut(sent))\n",
    "\n",
    "corpus_cn_seg = list(map(word_segment, corpus_cn))\n",
    "print(corpus_cn_seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_cn = CountVectorizer(token_pattern=r\"(?u)\\b\\w+\\b\")   # 改写token_pattern以匹配一个字符\n",
    "result_cn = vector_cn.fit_transform(corpus_cn_seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['不错', '中文', '今天', '今天天气', '处理', '工作', '忙', '星期二', '是', '最近', '男难', '自然语言', '非常']\n"
     ]
    }
   ],
   "source": [
    "print(vector_cn.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t1\n",
      "  (0, 3)\t1\n",
      "  (1, 6)\t1\n",
      "  (1, 12)\t1\n",
      "  (1, 5)\t1\n",
      "  (1, 9)\t1\n",
      "  (2, 10)\t1\n",
      "  (2, 4)\t1\n",
      "  (2, 11)\t1\n",
      "  (2, 1)\t1\n",
      "  (2, 12)\t1\n",
      "  (3, 7)\t1\n",
      "  (3, 8)\t1\n",
      "  (3, 2)\t1\n"
     ]
    }
   ],
   "source": [
    "print(result_cn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 1 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 1 0 0 1 0 0 1]\n",
      " [0 1 0 0 1 0 0 0 0 0 1 1 1]\n",
      " [0 0 1 0 0 0 0 1 1 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(result_cn.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**利用NLTK的FreqDist统计下词频**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'come', 'to', 'China', 'to', 'travel', 'This', 'is', 'a', 'car', 'polupar', 'in', 'China', 'I', 'love', 'tea', 'and', 'Apple', 'The', 'work', 'is', 'to', 'write', 'some', 'papers', 'in', 'science']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "corpus_list = []\n",
    "[corpus_list.extend(sent.split()) for sent in corpus]\n",
    "print(corpus_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('to', 3),\n",
       " ('I', 2),\n",
       " ('China', 2),\n",
       " ('is', 2),\n",
       " ('in', 2),\n",
       " ('come', 1),\n",
       " ('travel', 1),\n",
       " ('This', 1),\n",
       " ('a', 1),\n",
       " ('car', 1),\n",
       " ('polupar', 1),\n",
       " ('love', 1),\n",
       " ('tea', 1),\n",
       " ('and', 1),\n",
       " ('Apple', 1),\n",
       " ('The', 1),\n",
       " ('work', 1),\n",
       " ('write', 1),\n",
       " ('some', 1),\n",
       " ('papers', 1),\n",
       " ('science', 1)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(dict(nltk.FreqDist(corpus_list)).items(), key=lambda x:x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
