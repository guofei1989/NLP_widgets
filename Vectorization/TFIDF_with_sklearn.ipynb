{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "利用sklearn计算文档的Tf-Idf特征值, 与jieba利用内置或设定的idf字典库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 英文文本TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 原始文本\n",
    "document_en = [\"I have a pen.\",\n",
    "            \"I have an apple.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TfidfVectorizer(input='content', encoding='utf-8',\n",
    "                 decode_error='strict', strip_accents=None, lowercase=True,\n",
    "                 preprocessor=None, tokenizer=None, analyzer='word',\n",
    "                 stop_words=None, token_pattern=r\"(?u)\\b\\w\\w+\\b\",\n",
    "                 ngram_range=(1, 1), max_df=1.0, min_df=1,\n",
    "                 max_features=None, vocabulary=None, binary=False,\n",
    "                 dtype=np.float64, norm='l2', use_idf=True, smooth_idf=True,\n",
    "                 sublinear_tf=False))\n",
    "其主要参数和CountVectorizer基本一致，多的几个参数为：\n",
    "use_idf：默认True，即采用逆文档频率进行修正\n",
    "smooth_idf: 逆文本文档采用拉普拉斯平滑\n",
    "tokenizer: 分词器，None或callable对象，在analyzer='word'时可覆写\n",
    "\"\"\"\n",
    "vector = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2)\t0.5797386715376657\n",
      "  (0, 3)\t0.8148024746671689\n",
      "  (1, 2)\t0.4494364165239821\n",
      "  (1, 0)\t0.6316672017376245\n",
      "  (1, 1)\t0.6316672017376245\n"
     ]
    }
   ],
   "source": [
    "result1 = vector.fit_transform(document_en)\n",
    "print(result1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'an': 0, 'apple': 1, 'have': 2, 'pen': 3}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 词汇\n",
    "vector.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  中文文本TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "与英文文本处理的几点区别：<br>\n",
    "(1) 需要手动分词<br>\n",
    "(2) 可能需要修改token_pattern的正则表达式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_cn = [\"我是一条天狗呀！\",\"我把月来吞了。\",\"我把日来吞了。\",\"我把一切的星球来吞了。\",\"我把全宇宙来吞了。\",\"我便是我了！\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['我 是 一条 天狗 呀 ！',\n",
       " '我 把 月 来 吞 了 。',\n",
       " '我 把 日来 吞 了 。',\n",
       " '我 把 一切 的 星球 来 吞 了 。',\n",
       " '我 把 全宇宙 来 吞 了 。',\n",
       " '我 便是 我 了 ！']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 对于中文需要先分词\n",
    "import jieba\n",
    "\n",
    "def word_segment(sent):\n",
    "    return(\" \".join(jieba.lcut(sent)))\n",
    "\n",
    "text_seg = list(map(word_segment, document_cn))\n",
    "text_seg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector2 = TfidfVectorizer(max_features=10, token_pattern=r\"(?u)\\b\\w+\\b\")\n",
    "result2 = vector2.fit_transform(text_seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 7)\t0.2994779590123645\n",
      "  (0, 1)\t0.6746528559436284\n",
      "  (0, 6)\t0.6746528559436284\n",
      "  (1, 7)\t0.3463385195969726\n",
      "  (1, 8)\t0.46287181591384574\n",
      "  (1, 9)\t0.5401550231336202\n",
      "  (1, 5)\t0.46287181591384574\n",
      "  (1, 2)\t0.3997268378432121\n",
      "  (2, 7)\t0.41154075931008827\n",
      "  (2, 8)\t0.5500127990559459\n",
      "  (2, 5)\t0.5500127990559459\n",
      "  (2, 2)\t0.4749800471343644\n",
      "  (3, 7)\t0.2730597726436735\n",
      "  (3, 8)\t0.3649368050763702\n",
      "  (3, 9)\t0.4258683324651302\n",
      "  (3, 5)\t0.3649368050763702\n",
      "  (3, 2)\t0.3151521222301723\n",
      "  (3, 0)\t0.6151389439974322\n",
      "  (4, 7)\t0.2730597726436735\n",
      "  (4, 8)\t0.3649368050763702\n",
      "  (4, 9)\t0.4258683324651302\n",
      "  (4, 5)\t0.3649368050763702\n",
      "  (4, 2)\t0.3151521222301723\n",
      "  (4, 4)\t0.6151389439974322\n",
      "  (5, 7)\t0.6199649234576673\n",
      "  (5, 2)\t0.35776646893886044\n",
      "  (5, 3)\t0.6983170106657491\n"
     ]
    }
   ],
   "source": [
    "print(result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'我': 7, '一条': 1, '呀': 6, '把': 8, '来': 9, '吞': 5, '了': 2, '一切': 0, '全宇宙': 4, '便是': 3}\n"
     ]
    }
   ],
   "source": [
    "print(vector2.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**对于文档出现频繁的词反而不是很重要， 可以设定max_df参数**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 8)\t0.5\n",
      "  (0, 1)\t0.5\n",
      "  (0, 5)\t0.5\n",
      "  (0, 4)\t0.5\n",
      "  (1, 9)\t1.0\n",
      "  (2, 6)\t1.0\n",
      "  (3, 9)\t0.43968119520278715\n",
      "  (3, 0)\t0.6350907205215048\n",
      "  (3, 7)\t0.6350907205215048\n",
      "  (4, 9)\t0.5692126078464125\n",
      "  (4, 3)\t0.8221903715494888\n",
      "  (5, 2)\t1.0\n"
     ]
    }
   ],
   "source": [
    "vector2_2 = TfidfVectorizer(max_features=10, token_pattern=r\"(?u)\\b\\w+\\b\", max_df=0.6)\n",
    "result2_2 = vector2_2.fit_transform(text_seg)\n",
    "print(result2_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'是': 8, '一条': 1, '天狗': 5, '呀': 4, '来': 9, '日来': 6, '一切': 0, '星球': 7, '全宇宙': 3, '便是': 2}\n"
     ]
    }
   ],
   "source": [
    "print(vector2_2.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**对于 是 呀 等可设定停用词**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1)\t0.7071067811865476\n",
      "  (0, 4)\t0.7071067811865476\n",
      "  (1, 7)\t0.8221903715494888\n",
      "  (1, 8)\t0.5692126078464125\n",
      "  (2, 5)\t1.0\n",
      "  (3, 8)\t0.3711559310287487\n",
      "  (3, 0)\t0.5361104596573927\n",
      "  (3, 9)\t0.5361104596573927\n",
      "  (3, 6)\t0.5361104596573927\n",
      "  (4, 8)\t0.5692126078464125\n",
      "  (4, 3)\t0.8221903715494888\n",
      "  (5, 2)\t1.0\n"
     ]
    }
   ],
   "source": [
    "vector2_3 = TfidfVectorizer(max_features=10, token_pattern=r\"(?u)\\b\\w+\\b\", max_df=0.6, stop_words=[\"是\",\"呀\"])\n",
    "result2_3 = vector2_3.fit_transform(text_seg)\n",
    "print(result2_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'一条': 1, '天狗': 4, '月': 7, '来': 8, '日来': 5, '一切': 0, '的': 9, '星球': 6, '全宇宙': 3, '便是': 2}\n"
     ]
    }
   ],
   "source": [
    "print(vector2_3.vocabulary_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
