{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "利用gensim训练word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "from gensim import models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 载入文本数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/in_the_name_of_people.txt') as f:\n",
    "    document = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 分词处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/rk/nnl9yhm55kb6325ckffkn6hw0000gn/T/jieba.cache\n",
      "Loading model cost 0.666 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 添加强制分词\n",
    "jieba.suggest_freq('沙瑞金', True)\n",
    "jieba.suggest_freq('田国富', True)\n",
    "jieba.suggest_freq('高育良', True)\n",
    "jieba.suggest_freq('侯亮平', True)\n",
    "jieba.suggest_freq('钟小艾', True)\n",
    "jieba.suggest_freq('陈岩石', True)\n",
    "jieba.suggest_freq('欧阳菁', True)\n",
    "jieba.suggest_freq('易学习', True)\n",
    "jieba.suggest_freq('王大路', True)\n",
    "jieba.suggest_freq('蔡成功', True)\n",
    "jieba.suggest_freq('孙连城', True)\n",
    "jieba.suggest_freq('季昌明', True)\n",
    "jieba.suggest_freq('丁义珍', True)\n",
    "jieba.suggest_freq('郑西坡', True)\n",
    "jieba.suggest_freq('赵东来', True)\n",
    "jieba.suggest_freq('高小琴', True)\n",
    "jieba.suggest_freq('赵瑞龙', True)\n",
    "jieba.suggest_freq('林华华', True)\n",
    "jieba.suggest_freq('陆亦可', True)\n",
    "jieba.suggest_freq('刘新建', True)\n",
    "jieba.suggest_freq('刘庆祝', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\ufeff', ' ', '\\n', ' ', '\\n', ' ', '人民', '的', '名义', ' ']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 对于word2vec需谨慎做stopwords，因为其训练是依赖先后词的，但是一些符号也需要考虑进行去处\n",
    "# 这里为了方便，未做停用词处理\n",
    "document_cut = jieba.lcut(document)\n",
    "document_cut[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ufeff   \\n   \\n   人民 的 名义   \\n   周梅森   \\n   \\n   \\n   \\u3000 © 中文 在线 数字 出版 集团股份 有限公司 ， 2016 - 2017   \\n   \\u3000 数字 版图 书 '"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_sent = ' '.join(document_cut)\n",
    "document_sent[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/in_the_name_of_people_segment.txt', 'w') as f2:\n",
    "        f2.write(document_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 训练模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在gensim中，word2vec 相关的API都在包gensim.models.word2vec中。和算法有关的参数都在类gensim.models.word2vec.Word2Vec中。\n",
    "\n",
    "相关参数的说明：<br>\n",
    "sentences: 语料，可以是一个列表，或者从文件中遍历读出。 <br>\n",
    "size: 词向量的维度，默认100  <br>\n",
    "window: 词向量上下文最大距离，默认值为5， 对于一般的语料这个值推荐在5~10之间。 <br>\n",
    "sg: word2vec两个模型的选择。如果是0， 则是CBOW模型，是1则是Skip-Gram模型，默认是0即CBOW模型。 <br>\n",
    "hs: word2vec两个解法的选择。如果是0， 则是Negative Sampling，是1的话并且负采样个数negative大于0， 则是Hierarchical Softmax。默认是0即Negative Sampling。<br>\n",
    "negative: Negative Sampling时负采样的个数，默认是5。推荐在3~10。<br>\n",
    "cbow_mean: 仅用于CBOW在做投影的时候，若为0用上下文的词向量之和，若为1为上下文的词向量的平均值。<br>\n",
    "min_count: 需要计算词向量的最小词频。这个值可以去掉一些很生僻的低频词，默认是5。如果是小语料，可以调低这个值。<br>\n",
    "iter: 随机梯度下降法中迭代的最大次数，默认是5。对于大语料，可以增大这个值。<br>\n",
    "alpha: 在随机梯度下降法中迭代的初始步长。<br>\n",
    "min_alpha: 由于算法支持在迭代的过程中逐渐减小步长，min_alpha给出了最小的迭代步长值。对于大语料，需要对alpha, min_alpha,iter一起调参，来选择合适的三个值。<br>\n",
    "worker：线程数，默认1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-07-23 18:03:36,964 : INFO : collecting all words and their counts\n",
      "2019-07-23 18:03:36,966 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-07-23 18:03:37,026 : INFO : collected 17878 word types from a corpus of 161343 raw words and 2311 sentences\n",
      "2019-07-23 18:03:37,027 : INFO : Loading a fresh vocabulary\n",
      "2019-07-23 18:03:37,068 : INFO : effective_min_count=1 retains 17878 unique words (100% of original 17878, drops 0)\n",
      "2019-07-23 18:03:37,069 : INFO : effective_min_count=1 leaves 161343 word corpus (100% of original 161343, drops 0)\n",
      "2019-07-23 18:03:37,122 : INFO : deleting the raw counts dictionary of 17878 items\n",
      "2019-07-23 18:03:37,125 : INFO : sample=0.001 downsamples 38 most-common words\n",
      "2019-07-23 18:03:37,126 : INFO : downsampling leaves estimated 120578 word corpus (74.7% of prior 161343)\n",
      "2019-07-23 18:03:37,137 : INFO : constructing a huffman tree from 17878 words\n",
      "2019-07-23 18:03:37,533 : INFO : built huffman tree with maximum node depth 17\n",
      "2019-07-23 18:03:37,569 : INFO : estimated required memory for 17878 words and 100 dimensions: 33968200 bytes\n",
      "2019-07-23 18:03:37,570 : INFO : resetting layer weights\n",
      "2019-07-23 18:03:37,749 : INFO : training model with 3 workers on 17878 vocabulary and 100 features, using sg=0 hs=1 sample=0.001 negative=5 window=3\n",
      "2019-07-23 18:03:38,008 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-07-23 18:03:38,009 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-07-23 18:03:38,020 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-07-23 18:03:38,021 : INFO : EPOCH - 1 : training on 161343 raw words (120481 effective words) took 0.3s, 454137 effective words/s\n",
      "2019-07-23 18:03:38,231 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-07-23 18:03:38,233 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-07-23 18:03:38,248 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-07-23 18:03:38,249 : INFO : EPOCH - 2 : training on 161343 raw words (120410 effective words) took 0.2s, 531686 effective words/s\n",
      "2019-07-23 18:03:38,471 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-07-23 18:03:38,474 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-07-23 18:03:38,478 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-07-23 18:03:38,478 : INFO : EPOCH - 3 : training on 161343 raw words (120613 effective words) took 0.2s, 532097 effective words/s\n",
      "2019-07-23 18:03:38,689 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-07-23 18:03:38,690 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-07-23 18:03:38,705 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-07-23 18:03:38,706 : INFO : EPOCH - 4 : training on 161343 raw words (120718 effective words) took 0.2s, 536236 effective words/s\n",
      "2019-07-23 18:03:38,910 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-07-23 18:03:38,918 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-07-23 18:03:38,929 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-07-23 18:03:38,929 : INFO : EPOCH - 5 : training on 161343 raw words (120579 effective words) took 0.2s, 547349 effective words/s\n",
      "2019-07-23 18:03:38,931 : INFO : training on a 806715 raw words (602801 effective words) took 1.2s, 510148 effective words/s\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "sentences = models.word2vec.LineSentence('./data/in_the_name_of_people_segment.txt')   # 载入数据\n",
    "model = models.word2vec.Word2Vec(sentences, hs=1, min_count=1, window=3,size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 模型的应用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 近似词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-07-24 09:17:14,161 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('高育良', 0.9673596620559692),\n",
       " ('田国富', 0.9436954855918884),\n",
       " ('易学习', 0.9399048089981079),\n",
       " ('李达康', 0.9391453266143799),\n",
       " ('咱', 0.9371181726455688)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similar_by_word('沙瑞金', topn =5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "高育良 0.9673596620559692\n",
      "田国富 0.9436954855918884\n",
      "易学习 0.9399048089981079\n",
      "李达康 0.9391453266143799\n",
      "陆亦可 0.9217689037322998\n"
     ]
    }
   ],
   "source": [
    "req_count = 5\n",
    "for key in model.wv.similar_by_word('沙瑞金', topn =100):\n",
    "    if len(key[0])==3:\n",
    "        req_count -= 1\n",
    "        print(key[0], key[1])\n",
    "        if req_count == 0:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 两个词的相似度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9673596\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.similarity('沙瑞金','高育良'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 找不不同类的词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "刘庆祝\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/guofei/anaconda3/lib/python3.6/site-packages/gensim/models/keyedvectors.py:877: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.doesnt_match(u\"沙瑞金 高育良 李达康 刘庆祝\".split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 最相似的词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('高育良', 0.9673596620559692)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('沙瑞金')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 集合间的余弦相似度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.97881687\n"
     ]
    }
   ],
   "source": [
    "# 注意：当出现某个词语不在这个训练集合中的时候，会报错！\n",
    "list1 = ['沙瑞金','今天','政府'] \n",
    "list2 = ['高育良', '举报'] \n",
    "list_sim1 = model.wv.n_similarity(list1, list2)\n",
    "print(list_sim1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 模型保存和载入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-07-24 09:30:05,068 : INFO : saving Word2Vec object under ./word2vec_by_gensim, separately None\n",
      "2019-07-24 09:30:05,069 : INFO : not storing attribute vectors_norm\n",
      "2019-07-24 09:30:05,071 : INFO : not storing attribute cum_table\n",
      "2019-07-24 09:30:05,484 : INFO : saved ./word2vec_by_gensim\n"
     ]
    }
   ],
   "source": [
    "# 方法一：直接save，保存的文件不能利用文本编辑器查看但是保存了训练的全部信息，可以在读取后追加训练\n",
    "model.save('./word2vec_by_gensim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-07-24 09:30:18,998 : INFO : loading Word2Vec object from ./word2vec_by_gensim\n",
      "2019-07-24 09:30:19,273 : INFO : loading wv recursively from ./word2vec_by_gensim.wv.* with mmap=None\n",
      "2019-07-24 09:30:19,274 : INFO : setting ignored attribute vectors_norm to None\n",
      "2019-07-24 09:30:19,275 : INFO : loading vocabulary recursively from ./word2vec_by_gensim.vocabulary.* with mmap=None\n",
      "2019-07-24 09:30:19,276 : INFO : loading trainables recursively from ./word2vec_by_gensim.trainables.* with mmap=None\n",
      "2019-07-24 09:30:19,277 : INFO : setting ignored attribute cum_table to None\n",
      "2019-07-24 09:30:19,278 : INFO : loaded ./word2vec_by_gensim\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<gensim.models.word2vec.Word2Vec at 0x1a260d4f98>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 重新载入模型\n",
    "model2 = models.Word2Vec.load('./word2vec_by_gensim')\n",
    "model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 追加训练\n",
    "# model2.train(more_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 方法二：使用save_word2vec_format，可以设定用binary=True/Fasle存为二进制或纯文本\n",
    "# 方法保存为word2vec文本格式但是保存时丢失了词汇树等部分信息，不能追加训练\n",
    "# model.save_word2vec_format('/tmp/mymodel.txt',binary = False)\n",
    "# model.save_word2vec_format('/tmp/mymodel.bin.gz',binary = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
